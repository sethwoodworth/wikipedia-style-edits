Done:

* Come to terms with the immensity of the data
* For each 7z file, extract revision index

---
Jobs to run:

* (IN PROGRESS) For each 7z file, extract title index
* Given title and revision indices, combine them

* Draw a nice histogram of revisions per article

---

NOW:

* Write regexes in Python that corretly split or transform wiki markup

--

Code to write:

* Import all metadata into a MySQL db for fast access
** Hack mwdumper from mediawiki's CVS to not store actual content
   (requires SF.net anonymous CVS to not be broken)

* MXterminate the paragraphs 


---
Later:

* With mxterminated sentences, group 'em and diff 'em...
