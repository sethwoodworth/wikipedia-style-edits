Done:

* Come to terms with the immensity of the data

---
Jobs to run:

* For each 7z file, extract revision index
* (IN PROGRESS) For each 7z file, extract title index
* Given title and revision indices, combine them

* Draw a nice histogram of revisions per article

---

NOW:

* Fix split script! :-(
* Write regexes in Python that correctly split or transform wiki markup
** Alter this to correctly handle confusing tags plus wiki tables
** Link this to MXTERMINATOR
* Write a wrapper for MXTERMINATOR I can use smartly on a per-paragraph basis

--

Code to write:

* Import all metadata into a MySQL db for fast access
** Hack mwdumper from mediawiki's CVS to not store actual content
   (requires SF.net anonymous CVS to not be broken)

* MXterminate the paragraphs 


---
Later:

* With mxterminated sentences, group 'em and diff 'em...
